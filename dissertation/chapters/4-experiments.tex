\chapter{Experiments}
\label{ch.experiments}

% Neste capítulo
	This chapter evaluates the performance of communication services of
	the Nanvix Microkernel running on the \mppa processor, \ie \mailbox
	and \portal. The impacts of the synchronization mechanism were not
	analyzed because it is a simple service that does not directly
	influence node communication, depending greatly on the workload of
	each cluster. Noteworthy, the \sync was used in all benchmarks to
	synchronize the nodes involved due to the different boot times and
	the distinct node roles. The evaluation is divided into two sections.
	First, Section \autoref{sec.evaluation-methodology} describes the
	micro-benchmarks, their motivations, and the parameters used. Second,
	Section \autoref{sec.results-analysis} unveil and discusses our
	experimental results.

	\section{Evaluation Methodology}
	\label{sec.evaluation-methodology}

		To deliver a comprehensive assessment of the communication service, we
		stimulate the services with usual collective communication configurations.
		These configurations are usually found in distributed systems and present
		in the high-level services exported by Nanvix Multikernel. For instance,
		message exchanging between servers and clients, work distribution, and
		gathering results.

        Micro-benchmarks measure the data volume and communication latency
		through the \ioctl interface. In manycores, the nodes that communicate
		with peripherals are the bridge between the user and applications.
		Therefore, in our experiments, \iocluster plays the master role when a
		communication routine requires a master-slave behavior. \ioclusters also
		manages only one of the available interfaces to simplify communication.
		In all micro-benchmarks, only one \pe was used to request microkernel
		services.

		\subsection{Micro-benchmarks}

			To analyze the performance of the communication services, we
			relied in collective communication patters of \mpi, as well as
			common behaviors between clients and servers. The following
			subsections conceptually introduced each of these routines and
			behaviors.

			\subsubsection{Broadcast}

				\textit{Broadcast} is the most widely used communication pattern
				in \mpi. In this routine, a node sends the same data to
				all existing nodes. This process may be implemented in
				several ways, such as, Flat Tree, Binary Tree, Double Tree,
				and Chain~\cite{mpi-survey}. \autoref{fig:exp-broadcast}
				presents the \textit{Flat Tree} algorithm used in the benchmark.
				The Flat Tree defines that the root node should send data
				to everyone without delegating this function to other nodes.
				This routine can be used to send user inputs to a parallel
				program or to send configuration parameters to all
				nodes~\cite{url:mpitutorial}.

				\begin{figure}[!tb]
					\centering%
					\caption{Collective Communication Routines.}%
					\label{fig:mpi-routines}%

					\subcaptionminipage[fig:exp-broadcast]%
						{.35\linewidth}%
						{Example of \mpi Broadcast.}%
						{\includegraphics[width=.9\linewidth]{mpi-broadcast.pdf}}%
					\hspace{1cm}%
					\subcaptionminipage[fig:exp-gather]%
						{.35\linewidth}%
						{Example of \mpi Gather}%
						{\includegraphics[width=.9\linewidth]{mpi-gather.pdf}}%

					\vspace{0.5cm}%

					\subcaptionminipage[fig:exp-allgather]%
						{.35\linewidth}%
						{Example of \mpi AllGather.}%
						{\includegraphics[width=.9\linewidth]{mpi-allgather.pdf}}%
					\hspace{1cm}%
					\subcaptionminipage[fig:exp-ping-pong]%
						{.35\linewidth}%
						{Example of Ping-Pong.}%
						{\includegraphics[width=.9\linewidth]{mpi-ping-pong.pdf}}%

					\fonte{Adapted from \citeonline{url:mpitutorial}.}%
				\end{figure}

			\subsubsection{Gather}

				\textit{Gather} is the inverse operation of a broadcast variant
				called scatter. \autoref{fig:exp-gather} illustrates the reverse
				data flow, where this routine gathers the data distributed on a
				single node~\cite{url:mpitutorial}.	Similarly to broadcast, a
				Flat Tree was implemented where all root nodes send their parts
				directly to the root node.

			\subsubsection{AllGather}

				\textit{AllGather} is a routine that does not have a root node,
				illustrated by \autoref{fig:exp-allgather}. As the name suggests,
				the routine performs several Gather operations so that all
				participating nodes end with all pieces of data gathered. Some
				possible algorithms are Ring Algorithm, Recursive Doubling, Gather
				followed by Broadcast Algorithm. The benchmark implements the
				\textit{Bruck Algorithm} where each node will send its data to a
				node with distance $i$ and receive data from a distance $-i$ until
				all nodes contain the complete data.

			\subsubsection{Ping-Pong}

				Ping-Pong is not an \mpi collective communication routine but
				represents communication from a server answering requests from
				client nodes. \autoref{fig:exp-ping-pong} illustrates
				communication by focusing on the master node, where the master
				receives and answers one request at a time.

		\subsection{Experimental Design}
		\label{subsec:experimental-design}

			The parameters that we used for each micro-benchmark are detailed
			in Table~\ref{tab:benchmarks-parameters}.
		% Portal
			The first set of experiments sought to analyze the throughput
			provided by the Portal service. All micro-benchmarks involve 1
			\iocluster and 16 \cclusters, varying the size of the buffer to
			be transmitted from 4~KB to 64~KB. Larger values were not studied
			due to limitation on physical memory size in \cclusters (\ie 2~MB).
			For instance, AllGather requires approximately a total space of
			1~MB ($17$ nodes $\times$ 64~KB).
		% Mailbox
			The second set aimed to analyze the latency of the Mailbox
			service. The micro-benchmarks executed were practically the same
			as the Portal. However, the buffer size to be transmitted became
			constant, 120~Bytes. The variable parameter of the experiments was
			the number of \cclusters involved in the routines.
			Thus, \iocluster is always the master of routines, and the number
			of \ccluster is changed between 1 and 16.

		% Quantas iterações, limitações de memória e desvio padrão
			\mppa has intrinsic characteristics that guarantee low variability
			between runs. Thus, 50 iterations of each benchmark were performed.
			For each experiment, the first ten iterations were discarded to
			eliminate undesired warm-up effects. Finally, all results discussed
			bellow preset a standard error inferior to 1\%.

			\begin{table}[!tb]
				\centering%
				\caption{Micro-benchmark parameters for experiments.}%
				\label{tab:benchmarks-parameters}%

				\begin{tabular}{l|c|c|c|l|}
					\cline{2-5}
															 & \multicolumn{2}{c|}{\textbf{\portal}}   & \multicolumn{2}{c|}{\textbf{\mailbox}} \\ \cline{2-5}
															 & \textbf{Clusters} & \textbf{Data Size}  & \textbf{Clusters} & \textbf{Data Size} \\ \hline
					\multicolumn{1}{|l|}{\textbf{Broadcast}} & 1 IO, 16 CC       & 4, 8, 16, 32, 64 KB & 1 IO, 1 to 16 CC  & 120 B              \\ \hline
					\multicolumn{1}{|l|}{\textbf{Gather}}    & 1 IO, 16 CC       & 4, 8, 16, 32, 64 KB & 1 IO, 1 to 16 CC  & 120 B              \\ \hline
					\multicolumn{1}{|l|}{\textbf{AllGather}} & 1 IO, 16 CC       & 4, 8, 16, 32, 64 KB & 1 IO, 1 to 16 CC  & 120 B              \\ \hline
					\multicolumn{1}{|l|}{\textbf{Ping-Pong}} & 1 IO, 16 CC       & 4, 8, 16, 32, 64 KB & 1 IO, 1 to 16 CC  & 120 B              \\ \hline
				\end{tabular}

				\fonte{Developed by the author.}%
			\end{table}


	\section{Result Analysis}
	\label{sec.results-analysis}

		This is a introduction of the section.

		\subsection{Portal Throughput Analysis}

			\autoref{fig:exp-portal} presents the throughput of the Portal in MB/s
			relative to the different amounts of data transmitted. Results exhibit
			three distinct behaviors in the experiments. First, the Broadcast was
			expected to have the worst transmission rate due to the use of a single
			data transmitter. Since the measurement was done on the receiver side,
			the last slave had to wait for master transmits to all other nodes,
			considerably reducing the transfer rate in the Broadcast. Second, the
			Gather and Ping-Pong routines exhibited similar results, overlapping
			each other on the chart. This similarity is because the master node
			receives multiple requests and handles them serially one by one.
			The master node dictated the data flow in both benchmarks because
			transmission is only performed when allowed by the receiver. Finally,
			the AllGather routine exhibited the best results because of the
			parallelism of communications. Also, each communication pair co-occur,
			and multiple read/write requests not happen at the same time on a node,
			softening the interruption of the master core. In the context of \oss,
			we have subsystems requiring large data transfers, such as file and
			paging systems. In this case, observing the slope of the lines,
			we can infer that the 8~KB and 16~KB sizes favor Portal throughput.
			Overall, the results were as expected, but we believe that solving
			the problem with using DMA accelerators described in
			\autoref{sec.mppa-hardware-resources} could significantly improve
			Portal performance.

			\begin{figure}[!tb]
				\centering%
				\caption{Throughput of the Portal.}%
				\label{fig:exp-portal}%
				\includegraphics[width=.7\textwidth]{portal-throughput.pdf}%
				\fonte{Develop by the Author.}%
			\end{figure}

		\subsection{Mailbox Latency Analysis}

			\autoref{fig:exp-mailbox} presents the results of the experiments.
			Overall, the routines presented the expected behaviors.
			First, Gather routine, one of the essential routines, had the best
			results because receiving the messages occurs in parallel. Thus,
			the cost after the first message is the overhead of the service
			itself, not the communication. Second, AllGather routine exhibited
			similar behavior to Gather because all clusters send their messages
			before they start reading. Therefore the increase in latency is
			impacted by the transfer operation. The Broadcast routine also
			suffers from the same evil as the on Portal benchmark, where because
			exists only one node sending the messages impacts Mailbox latency.
			Finally, the linear behavior of the Ping-Pong routine is tailored
			by the overhead of sending messages to requesters. It can be noted
			that Ping-Pong has a slightly higher cost than the sum of Gather
			and Broadcast costs, where despite the benefits of receiving
			requests in parallel, the master spends most of his time handling
			requests sequentially.

			\todo[inline]{Sempre que você disser que o comportamento foi
				como o esperado, explicar antes o porque você esperava
					aquele comportamento.}

			\todo[inline]{Novamente, fazer um paralelo qualitativo dos
			resultados com o uso dessas primitivas no sistema. Ping-pong
			evidencia a latencia de comunicacao entre os subsistemas
			do Nanvix com o kernel remoto, mostrando um potencial
			para melhora com DMA pela plataforma. Broadcast, all
			gather e gather evidenciam como algoritmos
			distribuídos de concensus podem ser suportados
			de forma eficiente no multikernel.}

			\begin{figure}[!tb]
				\centering%
				\caption{Latency of the Mailbox.}%
				\label{fig:exp-mailbox}%
				\includegraphics[width=.7\textwidth]{mailbox-latency.pdf}%
				\fonte{Develop by the Author.}%
			\end{figure}
