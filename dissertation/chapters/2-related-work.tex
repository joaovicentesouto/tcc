\chapter{Related Work}
\label{ch.related-work}

In this chapter, we discuss research efforts related to this
undergraduate dissertation.
First, we present an overview of the state-of-the-art on
\lightweight \manycores. Then, due to the
lack of \oss focused on lightweight manycores, we will cover
different proposed \oss for manycores in general.

\section{Lightweight Manycore Processors}
\label{sec.works.manycores}

	Several research initiatives are focused on the design of
	\lightweight \manycores, aside from the \mppa \lightweight \manycore.
	For instance, \citeonline{olofsson2014} introduce
	\epiphany as a high-performance energy-efficient \manycore
	architecture suitable for real-time embedded systems.  The
	processor features multiple nodes interconnected through three 2D mesh
	\nocs with a distributed shared-memory model without coherence
	protocol. Each node has one \risc \cpu, multi-banked local memory,
	a \dma engine, an event monitor and a network interface.  The three
	\nocs are independent, scalable, and implement a packet-switched
	model with four duplex links at every node.

	\citeonline{Wallentowitz2013} presents the open-source \optimsoc framework
	to aid manycore processor design. The \optimsoc
	enables the rapid prototyping of a manycore, either via VHDL
	simulation or \fpga synthesis.
	In this architecture, several \openrisc cores\footnote{https://opencores.org/openrisc} are bundled into
	tiles, which in turn communicate through a packet-switched \noc.
	The \noc supports various network topologies, depending only on how the tiles are disposed on chip.
	Precisely, a \textit{network adapter} handles the memory transfers between
	a tile and its local memory and provides a message-passing communication
	model among tiles.
	Tiles can communicate by using message-exchange, partitioned global
	address space without cache coherence, or global memory
	with cache coherence via a write-through policy.

	Similarly, \citeonline{Kurth2017} introduces \hero, which
	groups an \arm host processor with a fully modifiable
	\riscv \manycore implemented on an \fpga. The \pmca uses a
	multi-cluster design and relies on multi-banked memory, called \spm.
	Data transfer occurs between a local \spm and all remote \spms or
	with shared global memory. Communication to main memory passes
	through software-managed lightweight \rab. The \rab performs the
	translation of the virtual-to-physical address, similarly to
	traditional \mmu, allowing clusters to share virtual address pointers.

\section{Operating Systems for Manycores}
\label{sec.works.os}

	\citeonline{Baumann2009} proposed a new \os design for scalable multicore
	systems, called \multikernel.
	In their perspective, the next-generation of \oss should embrace the networked nature
	of the machines, and thus borrow design ideas from large-scale distributed
	systems.
	Assuming that cores are independent nodes of a network, they build traditional
	\os functionalities as fully-featured processes on userspace.
	These processes communicate via message-passing and do not share the internal
	structures of the \os.
	The work showed how expensive it is to maintain a state of the \os through
	shared-memory instead of exchanging messages and the subsequent increase of
	the complexity of cache-coherence protocols.
	The \multikernel implementation, named Barelfish, follows three design principles.
	The first principle is to \textit{make all inter-core communication explicit}, which turns the system
	amenable to human or automated analysis because processes communicate only
	through well-defined interfaces.
	The second principle is to \textit{make \os structures hardware-neutral}, so the kernel
	code can be easy to debug and optimize, facilitating the deployment of \os for new
	processor types and avoiding rework.
	The third principle is to \textit{view \os state as replicated instead of shared}, which improves system
	scalability.

	In \citeonline{Wisniewski2014}, the concept of scalability was pushed
	to the extreme, towards \hpc.
	The principal motivation is the creation of an \os that simultaneously supports
	programmability through support \linux \api, and provides a lightweight kernel
	that achieves performance, scalability, and reliability.
	The \os, named \mos, provides as much of the hardware resources as
	possible to the \hpc applications and the \linux kernel component
	acts as a service that provides \linux functionalities.

	Similarly, \citeonline{kluge2014} developed the \moosca.
	With \moosca, they introduce abstractions that are easily composed, called Nodes,
	Channels, and Servers. Nodes represent execution resources, Channels represent communication
	resources, \eg \noc resources, and lastly, Servers are nodes that provide
	services to client Nodes.
	To meet safety-critical requirements, the \manycore is partitioned and each partition
	runs replicas of Servers, turning the whole system more predictable.
	However, in order to deal with interferences in shared resources,
	usage policies were introduced to make possible the prediction of system behavior.

	Finally, \citeonline{nightingale2009} presents  Helios, aiming to
	simplify the task of writing, deploying, and optimizing an application across
	heterogeneous cores.
	They use the microkernel model, naming \textit{satellite kernel}, to export
	a uniform and straightforward set of \os abstractions.
	The most important design decisions were to avoid unnecessary remote communication
	by thinking about the penalty they have in \numa domains.
	Moreover, it requests a minimum set hardware resources to support architectures with little
	computational power or memory constraints.

\section{Discussion}

	In \autoref{sec.works.manycores}, we discussed how \manycore architectures can be
	grouped over a common logic perspective.
	They all have one or more logical units distributed and incorporated on clusters.
	The clusters, interconnected through a network, communicate by message-exchange.
	However, due to the domain for which these processors were designed, they end up
	presenting several differences among them at the hardware level.

	Additionally, \autoref{sec.works.os} presented \oss studies that focus
	on the most efficient exploration of manycores processor characteristics.
	Many of them introduce entirely new concepts, reducing the programmability
	and portability of development environments. Some even seek to provide
	\posix interfaces by porting an adapted version of known kernels, but
	this can lead to optimization losses at near-hardware levels.
	However, the \os and communication models presented fit well with the
	distributed nature of manycores.
	Finally, many of these \oss work with \numa systems,
	where a complex bus system makes communication transparent. Therefore, there
	is, in fact, no network programming. 

	In this context, \nanvix \hal offers the ground \os abstractions needed to make
	\lightweight \manycores more easy to use and program.
	The exported interfaces sought to group lightweight manycores on a common and effective view.
	Above the \hal, services will be developed that seek first and foremost
	to provide greater programmability and portability through a fully-featured
	\posix-compliant \os. \nanvix is design specifically for manycores that require explicit
	programming of communication through \noc, have
	memory constraints and miss support for cache coherency. The combination of these
	features makes designing an \os for \lightweight \manycores challenging.