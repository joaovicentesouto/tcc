\chapter{Related Work}
\label{ch.related-works}

The proposal of this work is related to several other research works
on \textit{lightweight} \manycores.
First, some research papers describing state-of-the-art \textit{lightweight} \manycores
processors will be cited. Further, research on different \oses
proposed for such processors will be highlighted.

\section{Lightweight Manycore Processors}
\label{sec.works.manycores}

	In addition to \mppa, many works exemplify the wide variety of architectural
	possibilities of \textit{lightweight} \manycores.
	For instance, Olofsson \etal~\cite{olofsson2014} introduce \epiphany as a
	high-performance energy-efficient \manycore architecture suitable for
	real-time embedded systems.
	The architecture consists of nodes communicating through three 2D mesh \nocs
	with a distributed shared-memory model without coherence protocol.
	Each node has one \risc \cpu, multi-banked local memory, a \dma engine,
	an event monitor and a network interface.
	The three \nocs, named rMesh, cMesh, and xMesh, are independent and scalable.
	They implements a packet-switched model with four duplex links at every node.
	When a node want read a remote data, first, it sends a read request over the rMesh
	and wait a signal to arrive on cMesh.
	On the another hand, the write operation allows the node to continue running
	while data is tranfered through over xMesh.

	On the other hand, for help and facilitate on the \manycore processor design,
	Wallentowitz \etal~\cite{Wallentowitz2013} presents the open-source framework
	\optimsoc which allows build \manycore \soc and simulate them on a computer or
	synthesize them on a \fpga.
	The \pe are \openrisc~\footnote{https://opencores.org/openrisc}
	processor organized in tiles.
	The central architectural element is the LISNoC, a \textit{packet-switched \noc}
	that implements virtual channels to avoid message-dependent deadlocks.
	The LISNoC support various network topologies, depending only on the tiles organization.
	Precisely, a \textit{network adapter} handles the memory transfers between
	tile and the memory and provide hardware means to a message-passing communication
	model among tiles.
	The tiles organization and the network topology also allows handle communication
	in three different ways.
	First, when using distributed memory, communication is performed through the
	message-exchange between the tiles.
	Second, by using a partitioned global address space scattered through the
	tiles' local memories, the network adapter can perform \mpu-like memory
	address translation hiding the exchange of messages but without providing cache coherence.
	Finally, Wallentowitz \etal study policy of write-through snooping to provide
	a global memory shared among tiles with cache coherence.

	Similarly, Kurth \etal~\cite{Kurth2017} introduce the \hero, which unites an \arm
	host processor with a fully modifiable \riscv \manycore implemented on a \fpga.
	The \pmca uses a multi-cluster design and also relies on multi-banked memory, called \spm.
	The data caches had substituted to a multi-channel \dma engine that copy data
	between a shared L1 \spm and remote \spms or shared main memory.
	Communication to main memory passes through software-managed lightweight \rab.
	The \rab performs the translation of the virtual-to-physical address similar to an \mmu.
	In this way, clusters can share virtual address pointers.
	Besides, exists different designs for the shared instruction caches and
	top-level interconnection such as bus or \noc.

\section{Operating Systems for Lightweight Manycore Processors}
\label{sec.works.os}

	Baumann \etal~\cite{Baumann2009} proposed a new \os architecture for scalable multicore
	systems, called \multikernel.
	In their vision, the future of the \oses is on embracing the networked nature
	of the machines based on distributed systems ideas.
	Assuming the cores are independent nodes of a network, they build the tradicional
	\os functionalities as fully-featured processes.
	This processes communicate via message-passing and does not share the internal
	structures of the \os.
	The paper showed how expensive it is to maintain a state of the \os through
	shared-memory instead of exchanging messages and the subsequent increase of
	the complexity of cache-coherence protocols.
	The \multikernel implementation, named Barelfish, follow three design principles.
	First, \textit{Make all inter-core communication explicit} turns the system
	amenable to human or automated analysis because processes communicate only
	through well-defined interfaces.
	Second, \textit{Make \os structures hardware-neutral} makes the hardware-independent
	code easy to debug, optimize, and facilitates the deploy the \os for new
	processor types, avoiding rework.
	And lastly, \textit{View \os state as replicated instead of shared} improve system
	scalability.

	In Wisniewski~\cite{Wisniewski2014} \etal, the concept of scalability was pushed
	to the extreme, thinking on \hpc.
	The principal motivation is the creation of an \os that simultaneously supports
	programmability, through support \linux \api, and provides a lightweight kernel
	to performance, scalability, and reliability.
	The \os, named \mos, provide as much of the compute hardware resources as
	possible to the \hpc applications. On the other hand, the Linux kernel
	component acts as a service that provides Linux functionalities.

	In like manner, Kluge \etal~\cite{kluge2014} developed the \moosca.
	With \moosca, they introduce abstractions that are easily composed, called Nodes,
	Channels and, Servers.
	Where Nodes represent execution resources, Channels represent communication
	resources, \eg \noc resources, and lastly, Servers are nodes that provide
	services to client Nodes.
	To meet safety-critical requirements, they partition \manycore and distribute
	replicas of Servers, turning the whole system more predictable.
	However, in order to deal with interferences in shared resources,
	usage policies are introduced to make possible the prediction of system behavior.

	Finally, Nightingale \etal~\cite{nightingale2009} presents the Helios \os to
	simplify the process of writing, deploy, and optimize an application across
	heterogeneous cores.
	They use the microkernel model, naming \textit{satellite kernel}, to export
	a uniform and straightforward set of \os abstractions.
	The most important design decisions were to avoid unnecessary remote communication
	by thinking about the penalty they have in \numa domains.
	Also, request the minimum of hardware primitives so that architectures with many
	constraints can be ported.
	Moreover, request the minimum hardware resources to support architectures with little
	computational power or memory constraints.

\section{Discussion}

	Section \ref{sec.works.manycores} exemplifies how \manycore architectures can be
	grouped over a common logic vision.
	They all have one or more logical units distributed and incorporated on clusters.
	The clusters, interconnected through a network, communicate by message-exchange.
	However, due to the domain for which these processors were designed, they end up
	presenting several discrepancies among them at the hardware level.

	On the other hand, Section \ref{sec.works.os} presents studies of \oses that focus on
	mitigating these discrepancies in order to provide greater programmability and portability.
	Nevertheless, they span the entire development spectrum, delivering to end-users
	high-level abstractions for increase productivity.
	In this way, it is possible to notice the extensive rework that all of them had in
	dealing with the closest layer of hardware that, as observed, can be abstracted to
	a common vision.

	In this context, the proposed \hal deal with the lowest level possible with a focus
	on the aspects that make it challenging to work with \manycores.
	Thus, different \oses and services can be developed and disseminated by several
	architectures that implement such the vision.
	So consequently, the applications that run on top of them.